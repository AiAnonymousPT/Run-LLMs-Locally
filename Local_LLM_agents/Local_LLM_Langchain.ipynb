{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "1. Run local LLMs with Langchain <-\n",
    "2. Local agents\n",
    "3. Orchestrate multiple agents: CrewAi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1.Local LLMs with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  gpt4all\n",
    "\n",
    "# download a model from GPT4all, and put it in ./models/your_model.gguf (or any other extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the model with gpt4all\n",
    "local_path = (\"./models/mistral-7b-instruct-v0.1.Q4_0\")\n",
    " \n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True) # devcie='gpu' if possible (hardware/model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make template for prompting\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The capital of Russia, which spans across Europe and Asia, would be Moscow.\n",
      "2. However, if we consider only the landmass area (excluding water), Russia is not the largest country in both continents; it's actually Kazakhstan that holds this title.\n",
      "3. Therefore, the capital of the largest country in both Europe and Asia is Astana, which is located in Kazakhstan."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the capital of the largest country in both Europe and Asia?',\n",
       " 'text': \"1. The capital of Russia, which spans across Europe and Asia, would be Moscow.\\n2. However, if we consider only the landmass area (excluding water), Russia is not the largest country in both continents; it's actually Kazakhstan that holds this title.\\n3. Therefore, the capital of the largest country in both Europe and Asia is Astana, which is located in Kazakhstan.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity test: ask a question\n",
    "question = \"What is the capital of the largest country in both Europe and Asia?\"\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the model\n",
    "llm = Ollama(\n",
    "    model=\"llama2\", \n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest country in both Europe and Asia is Russia. The capital of Russia is Moscow."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the capital of the largest country in both Europe and Asia?',\n",
       " 'text': 'The largest country in both Europe and Asia is Russia. The capital of Russia is Moscow.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is the capital of the largest country in both Europe and Asia?\"\n",
    "llm_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
