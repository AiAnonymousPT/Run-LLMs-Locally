# Demos

Ollama - grab a quantized GGUF and create a custom Modelfile to run locally, see example code

- [ollama](ollama/)


Running in Production
- [HF Transformers + FastAPI on RunPod VM](https://github.com/mswaringen/transformers-fastapi)
- [Serverless Inference with Llama.CPP on RunPod](https://github.com/mswaringen/runpod-llama-serverless)