# Run LLMs Locally
Kick-off Meeting - March 14, 2024

Where & when: 

- Thursday, 14/03: 6h30pm - 8pm
- The  Fintech House [Address: Av. Duque de Loulé 12, Floor -1, 1050-093 Lisboa]

### **Interactive Discussion**

### 1.  Requirements for running LLMs locally

- **Hardware**: Discuss the computational resources needed to run LLMs locally
- **Model Formats & Quantization** Overview of different AI models formats and the role of quantization in reducing model size and accelerating inference

### 2. Inference Platforms

- **LM Studio (Basic)**: Intro to LM Studio for beginners, chat with a model quickly
- **Ollama (Medium)**: Run a server from the command line
- **Llama.cpp (Advanced)**: The OG, compile and configure for your system

### 3. Example Projects

- **Simple Local RAG**: Guide to implementing a basic RAG model and running locally
- **Simple Local Agents**: Instructions for developing local AI agents, including setting objectives and designing interactions.

//